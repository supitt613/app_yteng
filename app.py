import streamlit as st
import yt_dlp
import whisper
import os
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import Counter
import re
import tempfile
import shutil

# Download NLTK data if not present
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')

# --- Helper Functions ---

@st.cache_resource
def load_whisper_model(model_name="base"):
    """Loads the Whisper model, caching it for reuse."""
    return whisper.load_model(model_name)

def download_youtube_audio(url):
    """Downloads audio from a YouTube URL to a temporary file."""
    temp_dir = tempfile.mkdtemp()
    audio_path = os.path.join(temp_dir, "audio.mp3")
    ydl_opts = {
        'format': 'bestaudio/best',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
        'outtmpl': audio_path,
        'noplaylist': True,
        'quiet': True,
        'no_warnings': True,
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(url, download=True)
            video_title = info_dict.get('title', 'Unknown Title')
            return audio_path, video_title, temp_dir
    except Exception as e:
        st.error(f"Error downloading audio: {e}")
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        return None, None, None

def transcribe_audio(audio_path, model):
    """Transcribes audio using the Whisper model."""
    result = model.transcribe(audio_path)
    return result['text']

def clean_text_for_chunks(text):
    """Removes non-alphanumeric characters except spaces and converts to lowercase."""
    # Keep only letters, numbers, and spaces. Remove extra spaces.
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.lower()

def extract_language_chunks(text, num_chunks=7, min_len=2, max_len=4):
    """
    Extracts common N-gram language chunks from text.
    Filters for meaningful chunks (not just stop words, not starting/ending with stop words).
    """
    stop_words = set(stopwords.words('english'))
    cleaned_text = clean_text_for_chunks(text)
    words = word_tokenize(cleaned_text)

    all_candidate_chunks = []
    for n in range(min_len, max_len + 1):
        for i in range(len(words) - n + 1):
            chunk_words = words[i:i+n]
            chunk_str = " ".join(chunk_words)

            # Filter criteria for meaningful chunks:
            # 1. Contains at least one non-stop word
            # 2. Does not start or end with a stop word
            if (not any(word not in stop_words for word in chunk_words) or # If all are stop words
                chunk_words[0] in stop_words or
                chunk_words[-1] in stop_words):
                continue

            all_candidate_chunks.append(chunk_str)

    # Count frequency of candidate chunks
    chunk_counts = Counter(all_candidate_chunks)

    # Sort by frequency (descending)
    sorted_chunks = sorted(chunk_counts.items(), key=lambda item: item[1], reverse=True)

    # Select the top N chunks
    final_chunks = [chunk for chunk, _ in sorted_chunks[:num_chunks]]

    return final_chunks

# --- Streamlit App ---

st.set_page_config(layout="wide", page_title="YT Easy English Chunk Extractor")

st.title("ğŸ—£ï¸ YT Easy English èªå¡Šæ“·å–å™¨")
st.markdown("""
    è¼¸å…¥ YouTube å½±ç‰‡é€£çµï¼Œæ­¤æ‡‰ç”¨ç¨‹å¼å°‡æœƒï¼š
    1. å¾å½±ç‰‡ä¸­æ“·å–éŸ³è¨Šã€‚
    2. å°‡éŸ³è¨Šè½‰éŒ„æˆæ–‡å­—ã€‚
    3. å°‡è½‰éŒ„æ–‡æœ¬åˆ†å‰²æˆå°è©±ç‰‡æ®µã€‚
    4. å¾æ¯å€‹å°è©±ç‰‡æ®µä¸­æ‰¾å‡º 6-8 å€‹å¸¸ç”¨çš„è‹±èªèªå¡Š (phrases) ä¾›æ‚¨ç·´ç¿’ã€‚
    
    **æ³¨æ„ï¼š** æ­¤æ‡‰ç”¨ç¨‹å¼éœ€è¦æ‚¨çš„ç³»çµ±å®‰è£ `ffmpeg`ã€‚æ‚¨å¯ä»¥å¾ [ffmpeg.org](https://ffmpeg.org/download.html) ä¸‹è¼‰ä¸¦å®‰è£ã€‚
    """)

youtube_url = st.text_input("è«‹è¼¸å…¥ YouTube å½±ç‰‡é€£çµ (ä¾‹å¦‚: `https://www.youtube.com/watch?v=k_B_t1_d_24`) ", "")

if youtube_url:
    if "youtube.com/watch?v=" not in youtube_url and "youtu.be/" not in youtube_url:
        st.error("è«‹è¼¸å…¥æœ‰æ•ˆçš„ YouTube å½±ç‰‡é€£çµã€‚")
    else:
        st.video(youtube_url) # Display the video directly

        st.subheader("è™•ç†ä¸­...")
        progress_bar = st.progress(0)
        status_text = st.empty()

        # 1. Download Audio
        status_text.text("1/3 æ­£åœ¨ä¸‹è¼‰éŸ³è¨Š...")
        progress_bar.progress(33)
        audio_path, video_title, temp_dir = download_youtube_audio(youtube_url)

        if audio_path:
            st.success(f"å·²æˆåŠŸä¸‹è¼‰å½±ç‰‡: **{video_title}**")
            st.markdown(f"---")
            st.subheader(f"å½±ç‰‡æ¨™é¡Œ: {video_title}")

            # 2. Transcribe Audio
            status_text.text("2/3 æ­£åœ¨è½‰éŒ„éŸ³è¨Š (é€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“)... ")
            progress_bar.progress(66)
            try:
                model = load_whisper_model("base") # Using 'base' model for faster processing
                full_transcript = transcribe_audio(audio_path, model)
                st.success("éŸ³è¨Šè½‰éŒ„å®Œæˆï¼")
            except Exception as e:
                st.error(f"è½‰éŒ„éŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                full_transcript = None
            finally:
                # Clean up temporary audio file and directory
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)

            if full_transcript:
                status_text.text("3/3 æ­£åœ¨åˆ†æèªå¡Š...")
                progress_bar.progress(100)

                st.subheader("å®Œæ•´è½‰éŒ„æ–‡æœ¬ (å¯é¸)")
                with st.expander("é»æ“ŠæŸ¥çœ‹å®Œæ•´è½‰éŒ„æ–‡æœ¬"):
                    st.write(full_transcript)

                st.subheader("å°è©±ç‰‡æ®µèˆ‡å¸¸ç”¨èªå¡Š")

                # Segment the full transcript into sentences
                sentences = sent_tokenize(full_transcript)

                # Define dialogue segments (e.g., 3-5 sentences per segment)
                segment_size = 4 # Average number of sentences per segment
                num_chunks_per_segment = 7 # User requested 6-8

                dialogue_segments = []
                for i in range(0, len(sentences), segment_size):
                    segment_text = " ".join(sentences[i:i+segment_size])
                    dialogue_segments.append(segment_text)

                if not dialogue_segments:
                    st.warning("æœªèƒ½å¾è½‰éŒ„æ–‡æœ¬ä¸­åˆ†å‰²å‡ºå°è©±ç‰‡æ®µã€‚")
                else:
                    for i, segment in enumerate(dialogue_segments):
                        st.markdown(f"#### å°è©±ç‰‡æ®µ {i+1}")
                        st.info(segment) # Display the dialogue segment

                        # Extract chunks for this segment
                        chunks = extract_language_chunks(segment, num_chunks=num_chunks_per_segment)

                        if chunks:
                            st.markdown("**å»ºè­°ç·´ç¿’èªå¡Š:**")
                            cols = st.columns(3)
                            for j, chunk in enumerate(chunks):
                                cols[j % 3].success(f"ğŸ‘‰ {chunk}")
                        else:
                            st.warning("æœªèƒ½å¾æ­¤ç‰‡æ®µä¸­æ‰¾åˆ°å¸¸ç”¨èªå¡Šã€‚")
                        st.markdown("---")
            else:
                st.error("ç„¡æ³•é€²è¡Œèªå¡Šåˆ†æï¼Œå› ç‚ºè½‰éŒ„å¤±æ•—ã€‚")
        else:
            st.error("ç„¡æ³•é€²è¡Œèªå¡Šåˆ†æï¼Œå› ç‚ºéŸ³è¨Šä¸‹è¼‰å¤±æ•—ã€‚")

        progress_bar.empty()
        status_text.empty()
